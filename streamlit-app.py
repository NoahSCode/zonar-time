import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date, time
import altair as alt
import requests
import streamlit.components.v1 as components
from scipy.stats import linregress
from dataclasses import dataclass, field

################################################################################
# 0) Data Class for State Management (Unchanged)
################################################################################
@dataclass
class BusState:
    inside_origin: bool = False
    inside_dest: bool = False
    origin_arrival_time: pd.Timestamp = pd.NaT
    origin_depart_time: pd.Timestamp = pd.NaT
    dest_arrival_time: pd.Timestamp = pd.NaT
    prev_datetime: pd.Timestamp = pd.NaT
    prev_in_origin_zone: bool = False
    prev_in_dest_zone: bool = False
    prev_dist_traveled: float = np.nan
    origin_depart_start_dist: float = np.nan
    last_ping_in_origin_dt_current_trip: pd.Timestamp = pd.NaT

################################################################################
# 1) Utility: Haversine & Circle checks (Unchanged)
################################################################################
EARTH_RADIUS_FEET = 20902231

def haversine_distance_vectorized(lat1_arr: np.ndarray, lon1_arr: np.ndarray, lat2_scalar: float, lon2_scalar: float) -> np.ndarray:
    lat1_rad = np.radians(lat1_arr)
    lon1_rad = np.radians(lon1_arr)
    lat2_rad = np.radians(lat2_scalar)
    lon2_rad = np.radians(lon2_scalar)
    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad
    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    return EARTH_RADIUS_FEET * c

def interpolate_time(t1: pd.Timestamp, t2: pd.Timestamp) -> pd.Timestamp | None:
    if pd.isna(t1) or pd.isna(t2):
        return None
    t1 = pd.Timestamp(t1)
    t2 = pd.Timestamp(t2)
    midpoint = t1 + (t2 - t1) / 2
    return midpoint.replace(microsecond=0)

################################################################################
# 2) Optimized Hash Table Analysis (Unchanged)
################################################################################
def analyze_stop_crossings_hashtable(
    stops_df: pd.DataFrame, path_df: pd.DataFrame, origin_stop: str, destination_stop: str,
    radius_feet_origin: float = 200, radius_feet_destination: float = 200,
    op_day_cutoff_hour: int = 3
) -> pd.DataFrame:
    if stops_df.empty or path_df.empty:
        return pd.DataFrame()

    try:
        origin_row = stops_df[stops_df["Stop Name"] == origin_stop].iloc[0]
        dest_row = stops_df[stops_df["Stop Name"] == destination_stop].iloc[0]
    except IndexError:
        st.error(f"Origin '{origin_stop}' or Destination '{destination_stop}' not found in stops data.")
        return pd.DataFrame()

    df = path_df.sort_values("DateTime").copy()
    df['in_origin_zone'] = haversine_distance_vectorized(df["Lat"].values, df["Lon"].values, origin_row["Lat"], origin_row["Lon"]) <= radius_feet_origin
    df['in_dest_zone'] = haversine_distance_vectorized(df["Lat"].values, df["Lon"].values, dest_row["Lat"], dest_row["Lon"]) <= radius_feet_destination

    bus_states = {}
    all_results_list = []

    # Standardize column names for itertuples
    df.rename(columns={"Asset No.": "AssetNo", "Distance Traveled(Miles)": "DistanceTraveledMiles"}, inplace=True)

    for ping in df.itertuples(index=False):
        bus_id = ping.AssetNo
        curr_datetime = ping.DateTime
        curr_in_origin = ping.in_origin_zone
        curr_in_dest = ping.in_dest_zone
        curr_dist_traveled = getattr(ping, "DistanceTraveledMiles", np.nan)

        state = bus_states.get(bus_id)
        if state is None:
            state = BusState()
            bus_states[bus_id] = state

        if not state.inside_origin and curr_in_origin:
            state.inside_origin = True
            state.origin_arrival_time = curr_datetime
        
        if state.inside_origin and state.prev_in_origin_zone and not curr_in_origin:
            state.inside_origin = False
            state.origin_depart_time = interpolate_time(state.prev_datetime, curr_datetime)
            state.origin_depart_start_dist = state.prev_dist_traveled
            state.last_ping_in_origin_dt_current_trip = state.prev_datetime

        if not pd.isna(state.origin_depart_time) and not state.inside_dest and curr_in_dest:
            if curr_datetime < state.origin_depart_time:
                state.prev_datetime, state.prev_in_origin_zone, state.prev_in_dest_zone, state.prev_dist_traveled = curr_datetime, curr_in_origin, curr_in_dest, curr_dist_traveled
                continue

            state.inside_dest = True
            state.dest_arrival_time = curr_datetime
            
            actual_distance = curr_dist_traveled - state.origin_depart_start_dist if pd.notna(state.origin_depart_start_dist) and pd.notna(curr_dist_traveled) else 0.0
            origin_idle_mins = (state.origin_depart_time - state.origin_arrival_time).total_seconds() / 60.0
            travel_time_mins = (state.dest_arrival_time - state.origin_depart_time).total_seconds() / 60.0

            travel_date_str = None
            if pd.notnull(state.origin_depart_time):
                event_dt = state.origin_depart_time
                travel_date_to_format = event_dt.date() - timedelta(days=1) if event_dt.hour < op_day_cutoff_hour else event_dt.date()
                travel_date_str = travel_date_to_format.strftime("%m/%d/%Y")

            all_results_list.append({
                "Vehicle no.": bus_id, "Travel Date": travel_date_str,
                "Origin Stop": origin_stop, "Destination Stop": destination_stop,
                "Origin Stop Arrival": state.origin_arrival_time,
                "Origin Stop Departure": state.origin_depart_time,
                "Last Ping In Origin DateTime": state.last_ping_in_origin_dt_current_trip,
                "Origin Stop Idle (mins)": round(origin_idle_mins, 2),
                "Destination Stop Entry": state.dest_arrival_time,
                "Destination Stop Departure": pd.NaT,
                "Destination Stop Idle (mins)": None,
                "Actual Distance (miles)": round(actual_distance, 3) if actual_distance >= 0 else 0,
                "Travel Time": round(travel_time_mins, 2) if travel_time_mins >=0 else 0
            })
            
            state.origin_arrival_time, state.origin_depart_time, state.origin_depart_start_dist, state.last_ping_in_origin_dt_current_trip = pd.NaT, pd.NaT, np.nan, pd.NaT

        if state.inside_dest and state.prev_in_dest_zone and not curr_in_dest:
            dest_depart_time_val = interpolate_time(state.prev_datetime, curr_datetime)
            for i in range(len(all_results_list) - 1, -1, -1):
                record = all_results_list[i]
                if (record["Vehicle no."] == bus_id and pd.isna(record["Destination Stop Departure"]) and record["Destination Stop Entry"] == state.dest_arrival_time):
                    record["Destination Stop Departure"] = dest_depart_time_val
                    dest_idle_mins = (dest_depart_time_val - state.dest_arrival_time).total_seconds() / 60.0
                    record["Destination Stop Idle (mins)"] = round(dest_idle_mins, 2)
                    break
            state = BusState()
            bus_states[bus_id] = state

        state.prev_datetime, state.prev_in_origin_zone, state.prev_in_dest_zone, state.prev_dist_traveled = curr_datetime, curr_in_origin, curr_in_dest, curr_dist_traveled

    if not all_results_list:
        return pd.DataFrame()

    results_df = pd.DataFrame(all_results_list)
    datetime_cols = ["Origin Stop Arrival", "Origin Stop Departure", "Last Ping In Origin DateTime", "Destination Stop Entry", "Destination Stop Departure"]
    for col in datetime_cols:
        if col in results_df.columns:
            results_df[col] = pd.to_datetime(results_df[col], errors='coerce')
    
    final_cols_ordered = ["Vehicle no.", "Travel Date", "Origin Stop", "Destination Stop", "Origin Stop Arrival", "Origin Stop Departure", "Last Ping In Origin DateTime", "Origin Stop Idle (mins)", "Destination Stop Entry", "Destination Stop Departure", "Destination Stop Idle (mins)", "Actual Distance (miles)", "Travel Time"]
    return results_df[[col for col in final_cols_ordered if col in results_df.columns]]

################################################################################
# 3) Other Utility Functions (Unchanged)
################################################################################
def filter_by_gap_clustering_largest(df: pd.DataFrame, gap_threshold: float = 0.3) -> pd.DataFrame:
    if df.empty or "Actual Distance (miles)" not in df.columns: return df.copy()
    sorted_df = df.dropna(subset=["Actual Distance (miles)"]).sort_values(by="Actual Distance (miles)")
    if sorted_df.empty or len(sorted_df) < 2: return df.copy()
    distances, clusters_indices, current_cluster = sorted_df["Actual Distance (miles)"].values, [], [sorted_df.index[0]]
    if len(distances) == 1: clusters_indices.append(current_cluster)
    else:
        for i in range(len(distances) - 1):
            if (distances[i+1] - distances[i]) >= gap_threshold: clusters_indices.append(current_cluster); current_cluster = [sorted_df.index[i+1]]
            else: current_cluster.append(sorted_df.index[i+1])
        if current_cluster: clusters_indices.append(current_cluster)
    if not clusters_indices: return pd.DataFrame(columns=df.columns)
    return df.loc[max(clusters_indices, key=len)].copy()

def filter_idle_by_gap_clustering_largest(df: pd.DataFrame, idle_col: str, gap_threshold: float = 0.3) -> pd.DataFrame:
    if df.empty or idle_col not in df.columns: return df.copy()
    sub_df = df.dropna(subset=[idle_col])
    if sub_df.empty or len(sub_df) < 2: return df.copy()
    sorted_sub = sub_df.sort_values(by=idle_col)
    idle_vals, clusters_indices, current_cluster = sorted_sub[idle_col].values, [], [sorted_sub.index[0]]
    if len(idle_vals) == 1: clusters_indices.append(current_cluster)
    else:
        for i in range(len(idle_vals) - 1):
            if (idle_vals[i+1] - idle_vals[i]) >= gap_threshold: clusters_indices.append(current_cluster); current_cluster = [sorted_sub.index[i+1]]
            else: current_cluster.append(sorted_sub.index[i+1])
        if current_cluster: clusters_indices.append(current_cluster)
    if not clusters_indices: return pd.DataFrame(columns=df.columns)
    return df.loc[max(clusters_indices, key=len)].copy()

def filter_idle_by_iqr(df: pd.DataFrame, idle_col: str, multiplier: float = 1.5) -> pd.DataFrame:
    if df.empty or idle_col not in df.columns: return df.copy()
    sub = df.dropna(subset=[idle_col])
    if sub.empty: return df.copy()
    q1, q3 = sub[idle_col].quantile(0.25), sub[idle_col].quantile(0.75)
    iqr = q3 - q1
    upper_cut = q3 if iqr == 0 else q3 + multiplier * iqr
    return df[(df[idle_col] <= upper_cut) | df[idle_col].isna()].copy()

def snap_to_roads(coords: list[tuple[float, float]], api_key: str) -> list[tuple[float, float]]:
    if not coords: return []
    snapped_points, BATCH_SIZE = [], 100
    for i in range(0, len(coords), BATCH_SIZE):
        chunk = coords[i:i + BATCH_SIZE]
        path_param = "|".join(f"{c[0]},{c[1]}" for c in chunk)
        url = f"https://roads.googleapis.com/v1/snapToRoads?path={path_param}&interpolate=false&key={api_key}"
        try:
            resp = requests.get(url, timeout=10); resp.raise_for_status(); data = resp.json()
            for p in data.get("snappedPoints", []): snapped_points.append((p["location"]["latitude"], p["location"]["longitude"]))
        except requests.exceptions.RequestException as e: st.error(f"Snap to Roads API error: {e}"); return []
        except Exception as e: st.error(f"Error processing Snap to Roads response: {e}"); return []
    return snapped_points

def embed_snapped_polyline_map(original_coords, snapped_coords, origin_stop_lat, origin_stop_lon, dest_stop_lat, dest_stop_lon, radius_feet_origin, radius_feet_destination, api_key):
    # This function is long and unchanged, so it is omitted here for brevity but should be included in the final file.
    # It remains exactly as it was in the previous version.
    if not original_coords and not snapped_coords:
        st.write("No coordinates to display on map.")
        return
    radius_meters_origin = float(radius_feet_origin) * 0.3048
    radius_meters_dest = float(radius_feet_destination) * 0.3048
    center_lat, center_lon = (origin_stop_lat, origin_stop_lon) 
    if snapped_coords: center_lat, center_lon = snapped_coords[0]
    elif original_coords: center_lat, center_lon = original_coords[0]
    snapped_js = ",\n".join(f"{{ lat: {pt[0]}, lng: {pt[1]} }}" for pt in snapped_coords)
    orig_js = ",\n".join(f"{{ lat: {pt[0]}, lng: {pt[1]} }}" for pt in original_coords)
    # ... (rest of the HTML/JS code is the same)
    html_code = f"""
    <!DOCTYPE html><html><head><meta name="viewport" content="initial-scale=1.0, user-scalable=no" /><style>#map {{ height: 100%; width: 100%; }} html, body {{ margin: 0; padding: 0; height: 100%; }}</style></head><body><div id="map"></div><script>
    function initMap() {{ var map = new google.maps.Map(document.getElementById('map'), {{ zoom: 13, center: {{ lat: {center_lat}, lng: {center_lon} }} }});
    new google.maps.Circle({{ strokeColor: "#0000FF", strokeOpacity: 0.8, strokeWeight: 2, fillColor: "#0000FF", fillOpacity: 0.1, map: map, center: {{ lat: {origin_stop_lat}, lng: {origin_stop_lon} }}, radius: {radius_meters_origin} }});
    new google.maps.Marker({{ position: {{ lat: {origin_stop_lat}, lng: {origin_stop_lon} }}, map: map, title: "Origin Stop" }});
    new google.maps.Circle({{ strokeColor: "#00FF00", strokeOpacity: 0.8, strokeWeight: 2, fillColor: "#00FF00", fillOpacity: 0.1, map: map, center: {{ lat: {dest_stop_lat}, lng: {dest_stop_lon} }}, radius: {radius_meters_dest} }});
    new google.maps.Marker({{ position: {{ lat: {dest_stop_lat}, lng: {dest_stop_lon} }}, map: map, title: "Destination Stop" }});
    var snappedCoords = [{snapped_js}]; if (snappedCoords.length > 1) {{ var snappedRoute = new google.maps.Polyline({{ path: snappedCoords, geodesic: true, strokeColor: "#4285F4", strokeOpacity: 1.0, strokeWeight: 4 }}); snappedRoute.setMap(map); }}
    var originalCoords = [{orig_js}]; originalCoords.forEach(function(coord) {{ new google.maps.Marker({{ position: coord, map: map, icon: {{ path: google.maps.SymbolPath.CIRCLE, scale: 2, fillColor: '#FF0000', fillOpacity: 0.7, strokeColor: '#FF0000', strokeWeight: 1 }} }}); }}); }}
    </script><script async src="https://maps.googleapis.com/maps/api/js?key={api_key}&callback=initMap"></script></body></html>
    """
    components.html(html_code, height=600)


def calculate_time_of_day_adjusted_vectorized(s_depart_dt: pd.Series, s_travel_date_dt: pd.Series) -> pd.Series:
    # This function is long and unchanged, so it is omitted here for brevity but should be included in the final file.
    if s_depart_dt.empty or s_travel_date_dt.empty: return pd.Series(index=s_depart_dt.index, dtype=float)
    depart_dt = pd.to_datetime(s_depart_dt, errors='coerce')
    time_val = depart_dt.dt.hour + depart_dt.dt.minute / 60.0
    travel_date_dt_ts = pd.to_datetime(s_travel_date_dt)
    departure_date_only = depart_dt.dt.date
    is_one_day_after = (departure_date_only > s_travel_date_dt) & (departure_date_only <= (s_travel_date_dt + pd.to_timedelta(1, unit='D')))
    is_two_days_after = (departure_date_only > (s_travel_date_dt + pd.to_timedelta(1, unit='D'))) & (departure_date_only <= (s_travel_date_dt + pd.to_timedelta(2, unit='D')))
    time_val = np.where(is_one_day_after, time_val + 24.0, time_val)
    time_val = np.where(is_two_days_after, time_val + 48.0, time_val)
    time_val[depart_dt.isna() | travel_date_dt_ts.isna()] = np.nan
    return pd.Series(time_val, index=s_depart_dt.index)

def format_time_of_day_label(time_val_24hr: float) -> str:
    # This function is long and unchanged, so it is omitted here for brevity but should be included in the final file.
    if pd.isna(time_val_24hr): return "N/A"
    day_offset = int(time_val_24hr // 24)
    hour_of_day_24 = time_val_24hr % 24
    hour_int, minute_fraction = int(hour_of_day_24), hour_of_day_24 - int(hour_of_day_24)
    minute_int = int(round(minute_fraction * 60))
    if minute_int == 60: hour_int += 1; minute_int = 0;
    if hour_int == 24: hour_int = 0; day_offset += 1;
    period, hour_12 = "AM", hour_int
    if hour_12 == 0: hour_12 = 12
    elif hour_12 == 12: period = "PM"
    elif hour_12 > 12: hour_12 -= 12; period = "PM";
    label = f"{hour_12}:{minute_int:02d} {period}"
    if day_offset == 1: label += " (Next Day)"
    elif day_offset > 1: label += f" (+{day_offset} Days)"
    return label

################################################################################
# 7) Main App (MODIFIED FOR MULTIPLE FILE UPLOADS)
################################################################################

def main():
    st.set_page_config(layout="wide") 
    st.title("Zonar Stop Time Analysis (Multi-File Upload Capable)")

    st.sidebar.header("Analysis Configuration")
    op_day_cutoff_hour_config = st.sidebar.number_input("Operational Day Cutoff Hour (e.g., 3 for 3 AM)", 0, 23, 3, 1)
    google_maps_api_key = st.sidebar.text_input("Google Maps API Key (Optional)", type="password")

    # Initialize session state
    if "stops_df" not in st.session_state: st.session_state.stops_df = pd.DataFrame()
    if "raw_path_df" not in st.session_state: st.session_state.raw_path_df = pd.DataFrame()
    if "path_df" not in st.session_state: st.session_state.path_df = pd.DataFrame()
    if "filtered_df" not in st.session_state: st.session_state.filtered_df = pd.DataFrame()
    # Other session state keys are initialized as needed later...

    c1_file, c2_file = st.columns(2)
    # --- MODIFICATION 1: Allow multiple stops files ---
    stops_files = c1_file.file_uploader(
        "Upload Stops Classification CSV(s)",
        type=["csv"],
        accept_multiple_files=True
    )
    # --- MODIFICATION 2: Allow multiple path files ---
    path_files = c2_file.file_uploader(
        "Upload Zonar Data CSV(s) (e.g., one file per month)",
        type=["csv"],
        accept_multiple_files=True
    )

    # --- MODIFICATION 3: Logic to process multiple stops files ---
    if stops_files and st.session_state.stops_df.empty:
        list_of_stops_dfs = []
        try:
            for uploaded_file in stops_files:
                stops_dtypes = {"Stop Name": "str", "Lat": "float64", "Lon": "float64", "Stop ID": "str"}
                df = pd.read_csv(uploaded_file, dtype=stops_dtypes, usecols=list(stops_dtypes.keys()))
                list_of_stops_dfs.append(df)
            
            if list_of_stops_dfs:
                stops_df_loaded = pd.concat(list_of_stops_dfs, ignore_index=True)
                required_stop_cols = ["Stop Name", "Lat", "Lon", "Stop ID"]
                if not set(required_stop_cols).issubset(stops_df_loaded.columns):
                    st.error(f"Stops CSVs must have columns: {', '.join(required_stop_cols)}")
                else:
                    stops_df_loaded.drop_duplicates(inplace=True)
                    stops_df_loaded['_sort_stop_id_'] = pd.to_numeric(stops_df_loaded['Stop ID'], errors='coerce')
                    stops_df_loaded.sort_values(by=['_sort_stop_id_', 'Stop Name'], inplace=True, na_position='last')
                    stops_df_loaded.drop(columns=['_sort_stop_id_'], inplace=True)
                    stops_df_loaded.reset_index(drop=True, inplace=True)
                    stops_df_loaded["Stop_Name_ID"] = stops_df_loaded["Stop Name"] + " (" + stops_df_loaded["Stop ID"].astype(str) + ")"
                    st.session_state["stops_df"] = stops_df_loaded
                    st.success(f"Loaded {len(stops_df_loaded)} unique stops from {len(stops_files)} file(s).")
        except Exception as e:
            st.error(f"Error loading stops CSV(s): {e}")
            st.session_state["stops_df"] = pd.DataFrame()

    # --- MODIFICATION 4: Logic to process multiple path files ---
    if path_files and st.session_state.raw_path_df.empty:
        list_of_path_dfs = []
        try:
            with st.spinner(f"Loading data from {len(path_files)} file(s)..."):
                for uploaded_file in path_files:
                    st.info(f"Processing {uploaded_file.name}...")
                    df = pd.read_csv(uploaded_file) # Read first, then process
                    
                    rename_map = {
                        "Asset No.": "AssetNo",
                        "Distance Traveled(Miles)": "DistanceTraveledMiles",
                        "Time(EST)": "TimeEST",
                        "Time(EDT)": "TimeEDT"
                    }
                    df.rename(columns=rename_map, inplace=True)
                    
                    essential_path_cols = ["AssetNo", "Date", "DistanceTraveledMiles", "Lat", "Lon"]
                    if not set(essential_path_cols).issubset(df.columns):
                        st.warning(f"Skipping file {uploaded_file.name} - missing required columns.")
                        continue

                    time_col_to_use = "TimeEDT" if "TimeEDT" in df.columns and df["TimeEDT"].notna().any() else "TimeEST"
                    df["DateTime"] = pd.to_datetime(df["Date"] + " " + df[time_col_to_use], errors="coerce")
                    df.dropna(subset=["DateTime"], inplace=True)
                    
                    df["AssetNo"] = df["AssetNo"].astype("category")
                    df["Lat"] = df["Lat"].astype("float64")
                    df["Lon"] = df["Lon"].astype("float64")
                    df["DistanceTraveledMiles"] = df["DistanceTraveledMiles"].astype("float32")
                    
                    list_of_path_dfs.append(df[["AssetNo", "DateTime", "DistanceTraveledMiles", "Lat", "Lon"]])
            
            if list_of_path_dfs:
                combined_path_df = pd.concat(list_of_path_dfs, ignore_index=True)
                # Rename columns back for compatibility with analysis function
                combined_path_df.rename(columns={"AssetNo": "Asset No.", "DistanceTraveledMiles": "Distance Traveled(Miles)"}, inplace=True)
                st.session_state["raw_path_df"] = combined_path_df
                st.success(f"Loaded a total of {len(combined_path_df)} path records from {len(list_of_path_dfs)} valid file(s).")
            else:
                st.warning("No valid path data could be loaded from the provided files.")

        except Exception as e:
            st.error(f"A critical error occurred while loading path CSV(s): {e}")
            st.session_state["raw_path_df"] = pd.DataFrame()

    # The rest of the app logic for filtering and analysis remains the same,
    # as it operates on the final, combined DataFrames in session_state.
    
    # Date Filtering (unchanged, operates on st.session_state.raw_path_df)
    if not st.session_state.get("raw_path_df", pd.DataFrame()).empty:
        path_df_for_filtering = st.session_state["raw_path_df"]
        min_dt_available = path_df_for_filtering["DateTime"].min().date()
        max_dt_available = path_df_for_filtering["DateTime"].max().date()
        # ... (rest of date filtering logic is identical)
        st.sidebar.markdown("**Filter Zonar Data by Date Range:**")
        s_filt = st.sidebar.date_input("Start date", value=min_dt_available, min_value=min_dt_available, max_value=max_dt_available)
        e_filt = st.sidebar.date_input("End date", value=max_dt_available, min_value=min_dt_available, max_value=max_dt_available)
        if s_filt and e_filt:
            if s_filt <= e_filt:
                start_datetime, end_datetime = datetime.combine(s_filt, time.min), datetime.combine(e_filt, time.max)
                st.session_state["path_df"] = path_df_for_filtering[(path_df_for_filtering["DateTime"] >= start_datetime) & (path_df_for_filtering["DateTime"] <= end_datetime)].copy()
                st.sidebar.success(f"{len(st.session_state['path_df'])} records after date filter.")
            else:
                st.sidebar.error("Start date cannot be after end date.")
                st.session_state["path_df"] = path_df_for_filtering.copy()
    else:
        st.session_state["path_df"] = pd.DataFrame()

    # Analysis Setup UI (unchanged)
    # ... This entire block remains the same, as it pulls from session_state ...
    stops_ui = st.session_state.stops_df
    path_analysis_df = st.session_state.path_df
    if not stops_ui.empty and not path_analysis_df.empty:
        st.header("Stop Analysis Setup")
        # ... (all selectbox and number_input UI is identical) ...
        # ... (the "Analyze" button and subsequent filtering logic is identical) ...
        col1, col2 = st.columns(2)
        stop_name_id_list = stops_ui["Stop_Name_ID"].unique().tolist()
        orig_choice_name_id = col1.selectbox("Origin Stop:", stop_name_id_list, index=0, key="sb_origin")
        dest_choice_name_id = col2.selectbox("Destination Stop:", stop_name_id_list, index=min(1, len(stop_name_id_list)-1), key="sb_destination")
        actual_origin_stop_name = stops_ui.loc[stops_ui["Stop_Name_ID"] == orig_choice_name_id, "Stop Name"].iloc[0]
        actual_dest_stop_name = stops_ui.loc[stops_ui["Stop_Name_ID"] == dest_choice_name_id, "Stop Name"].iloc[0]
        r_orig_val = col1.number_input("Origin Radius (ft)", min_value=1, value=200, step=10, key="num_r_orig")
        r_dest_val = col2.number_input("Destination Radius (ft)", min_value=1, value=200, step=10, key="num_r_dest")
        
        st.sidebar.header("Filtering Options")
        max_dist_filter = st.sidebar.number_input("Max Trip Distance (miles, 0=auto cluster)", 0.0, value=0.0, format="%.2f", step=0.5)
        gap_dist_filter = st.sidebar.number_input("Distance Cluster Gap (miles)", 0.01, value=0.3, format="%.2f", step=0.05)
        dwell_gap_filter = st.sidebar.number_input("Dwell Time Cluster Gap (mins)", 0.01, value=0.3, format="%.2f", step=0.05)
        iqr_mult_filter = st.sidebar.number_input("IQR Dwell Multiplier", 0.1, value=1.5, format="%.1f", step=0.1)
        rm_tt_15_filter = st.sidebar.checkbox("Remove Top 15% Travel Time Outliers?", value=False)
        
        if st.button("ðŸš€ Analyze Stop Crossings", type="primary", use_container_width=True):
            if actual_origin_stop_name == actual_dest_stop_name: st.error("Origin and Destination stops cannot be the same.")
            else:
                with st.spinner("Analyzing stop crossings..."):
                    res_df = analyze_stop_crossings_hashtable(stops_ui, path_analysis_df, actual_origin_stop_name, actual_dest_stop_name, r_orig_val, r_dest_val, op_day_cutoff_hour=op_day_cutoff_hour_config)
                # ... (rest of filtering chain is identical) ...
                if not res_df.empty:
                    final_df = res_df.copy()
                    st.write(f"Initial trips found: {len(final_df)}")
                    if max_dist_filter > 0: final_df = final_df[final_df["Actual Distance (miles)"] <= max_dist_filter]
                    else: final_df = filter_by_gap_clustering_largest(final_df, gap_dist_filter)
                    st.write(f"After distance filter: {len(final_df)}")
                    # ... and so on for all filters
                    st.session_state.filtered_df = final_df.reset_index(drop=True)
    
    # Display results (unchanged)
    # ... This entire block remains the same, as it pulls from session_state ...
    display_df_final = st.session_state.get("filtered_df", pd.DataFrame())
    if not display_df_final.empty:
        st.header(f"ðŸ“Š Analysis Results: {len(display_df_final)} Trips")
        # ... (all dataframe display, metrics, and plotting is identical) ...


# Helper functions for plotting and checks (`_check_empty_and_stop`, `_safe_round`, `_plot_dist`, `_plot_scatter_tod`)
# are unchanged and omitted for brevity. They should be included in the final file.
def _check_empty_and_stop(df: pd.DataFrame, filter_name: str) -> bool:
    if df.empty:
        st.warning(f"All trips removed by {filter_name} filter.")
        st.session_state["filtered_df"] = pd.DataFrame() 
        st.stop()
    return False

def _safe_round(val, decimals=2):
    if isinstance(val, (int, float, np.number)) and pd.notnull(val):
        return round(float(val), decimals)
    return "N/A"

# ... include the rest of the helper functions ...

if __name__ == "__main__":
    main()